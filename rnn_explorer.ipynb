{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sN8-G5ZQaeKW",
        "bPst9urDiIpz",
        "HswRk8xkiMqr",
        "kvjuGomWkkha",
        "54CAgWV0mrmu",
        "52I2c89Wm0C1",
        "_qq9N1Bxobyr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generating text using an RNN"
      ],
      "metadata": {
        "id": "9KwvqBNraYWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why would you use an RNN instead of (other technique)?"
      ],
      "metadata": {
        "id": "xSHp3GzbamKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural language and text generation these days is mostly done with transformers (see GPT-2 and GPT-3, for example), but I still wanted to explore recurrent neural networks, so I decided to try doing some text generation."
      ],
      "metadata": {
        "id": "-3iBH5uUalKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Licensing information"
      ],
      "metadata": {
        "id": "sN8-G5ZQaeKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2023 Simon Wu\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ],
      "metadata": {
        "id": "XRH3AwRIYtNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parts of this notebook were created by the Tensorflow Authors under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0)."
      ],
      "metadata": {
        "id": "2MFtoluoaSMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The blog post corpus used was [The Blog Authorship Corpus](https://u.cs.biu.ac.il/~koppel/BlogCorpus.htm). No particular license for use is noted, but in the words of the creators of the corpus:\n",
        "\n",
        "> The corpus may be freely used for non-commercial research purposes."
      ],
      "metadata": {
        "id": "7YpfE1OcaTRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Shakespeare corpus used was scraped by [Andrej Karpathy](https://github.com/karpathy/char-rnn). The works of Shakespeare are in the public domain, and Karpathy's repository is under the MIT license."
      ],
      "metadata": {
        "id": "jr4EilqVa2IK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recipe data was scraped from the [based.cooking](https://github.com/LukeSmithxyz/based.cooking/) GitHub repository. The contents of the site and repository were placed into the public domain by its authors."
      ],
      "metadata": {
        "id": "WITDrHt6bdZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before you start"
      ],
      "metadata": {
        "id": "ebJKmHAYcLde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to poke around with the code in this notebook, please make sure to enable GPU on Colab or whatever runtime you use for the notebook."
      ],
      "metadata": {
        "id": "su-Dnp4VcMv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic architecture"
      ],
      "metadata": {
        "id": "scR0gPKCcjFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These models were trained as recurrent neural networks. They take in tokenized text and output a probability distribution of the possible next character in the form of logits."
      ],
      "metadata": {
        "id": "1NvSL04zcl_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most likely next character is selected, appended to the beginning prompt, and then the text is taken in by the model again to generate the next character after that."
      ],
      "metadata": {
        "id": "ZDx5zHsJnIm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three layers are used:\n",
        "- Embedding layer, turning input text into one-hot vectors\n",
        "- GRU RNN layer, which is trained to predict the next characters\n",
        "- Output layer that gives logits (log probabilities)"
      ],
      "metadata": {
        "id": "FdWxbWZbnJQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "FZub3EAydrD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize"
      ],
      "metadata": {
        "id": "u5KizLqVdsZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import needed libraries and data"
      ],
      "metadata": {
        "id": "bPst9urDiIpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing needed libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import shutil\n",
        "import zipfile\n",
        "import requests"
      ],
      "metadata": {
        "id": "iOPFn6OHdvDf"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting datasets\n",
        "\n",
        "shakes_data = tf.keras.utils.get_file('shakespeare.txt', 'https://raw.githubusercontent.com/shangmingwu/rnn-text-generator/main/data/shakespeare-corpus.txt')\n",
        "recipe_data = tf.keras.utils.get_file('recipes.txt', 'https://raw.githubusercontent.com/shangmingwu/rnn-text-generator/main/data/recipes-corpus.txt')\n",
        "blog_data = tf.keras.utils.get_file('blogs.txt', 'https://raw.githubusercontent.com/shangmingwu/rnn-text-generator/main/data/blog-corpus.txt')"
      ],
      "metadata": {
        "id": "NSo5HeOXfytI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the text from the datasets\n",
        "\n",
        "shakes_text = open(shakes_data, 'rb').read().decode(encoding = 'utf-8')\n",
        "recipe_text = open(recipe_data, 'rb').read().decode(encoding = 'utf-8')\n",
        "blog_text = open(blog_data, 'rb').read().decode(encoding = 'utf-8')"
      ],
      "metadata": {
        "id": "dl01IYHbg7td"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vocabulary: an entry for each character in the text\n",
        "\n",
        "vocab = sorted(set(shakes_text + recipe_text + blog_text))"
      ],
      "metadata": {
        "id": "JTD4mJ2MhZwo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process text for training"
      ],
      "metadata": {
        "id": "HswRk8xkiMqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Methods for converting to and from tensors\n",
        "\n",
        "chars_to_tensor = tf.keras.layers.StringLookup(vocabulary = list(vocab), mask_token = None)\n",
        "tensor_to_chars = tf.keras.layers.StringLookup(vocabulary = list(vocab), invert = True, mask_token = None)\n",
        "\n",
        "def tensor_to_string(tokens):\n",
        "  return tf.strings.reduce_join(tensor_to_chars(tokens), axis = -1)"
      ],
      "metadata": {
        "id": "B7V-IX2IiUPA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select desired dataset"
      ],
      "metadata": {
        "id": "1yuwpq6wjqBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code block to use Shakespeare data.\n",
        "\n",
        "text = shakes_text\n",
        "prompt = 'ROMEO: '"
      ],
      "metadata": {
        "id": "bDDqOSPyjtjF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code block to use recipe data.\n",
        "\n",
        "text = recipe_text\n",
        "prompt = '---'"
      ],
      "metadata": {
        "id": "KChDJVHYj1YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code block to use blog data.\n",
        "\n",
        "text = blog_text\n",
        "prompt = '<Blog>'"
      ],
      "metadata": {
        "id": "5KV8b_GLj848"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training datasets"
      ],
      "metadata": {
        "id": "kvjuGomWkkha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the chosen dataset into a lot of tensors\n",
        "\n",
        "all_tensors = chars_to_tensor(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "tensor_dataset = tf.data.Dataset.from_tensor_slices(all_tensors)"
      ],
      "metadata": {
        "id": "DEYENjdfkmKO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into sequences of tensors\n",
        "\n",
        "sequence_length = 100\n",
        "sequences = tensor_dataset.batch(sequence_length + 1, drop_remainder = True)\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "aIoUa1ICk_FP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle data around into batches\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Tensorflow uses a buffer for shuffling\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder = True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "metadata": {
        "id": "mESh6HzVl4C8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the RNN"
      ],
      "metadata": {
        "id": "54CAgWV0mrmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the model"
      ],
      "metadata": {
        "id": "52I2c89Wm0C1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars_to_tensor.get_vocabulary())\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_dim = 1024"
      ],
      "metadata": {
        "id": "e7DBZ_dzm2Fe"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_dim):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_dim,\n",
        "                                   return_sequences = True,\n",
        "                                   return_state = True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states = None, return_state = False, training = False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "fn3RIVOvndDK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextTraining(TextModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training = True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "pSwi_El1odJj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextTraining(\n",
        "    vocab_size = len(chars_to_tensor.get_vocabulary()),\n",
        "    embedding_dim = embedding_dim,\n",
        "    rnn_dim = rnn_dim)"
      ],
      "metadata": {
        "id": "hAK4bRu4ntOI"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True))"
      ],
      "metadata": {
        "id": "bL3NP0Xfn_2C"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "_qq9N1Bxobyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Amount of training periods\n",
        "\n",
        "## Per-period training time can vary from 15 to 30 seconds depending on dataset\n",
        "\n",
        "## If you don't like the output, increasing epochs might help\n",
        "\n",
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "bt6Y5T1jpiz5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "8vu3g1C5pvqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, tensor_to_chars, chars_to_tensor, temperature = 1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.tensor_to_chars = tensor_to_chars\n",
        "    self.chars_to_tensor = chars_to_tensor\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.chars_to_tensor(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(chars_to_tensor.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.chars_to_tensor(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.tensor_to_chars(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "uUT0YAh8qCBW"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "single_step = OneStep(model, tensor_to_chars, chars_to_tensor)"
      ],
      "metadata": {
        "id": "_HV5nMiFqpmA"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate text!"
      ],
      "metadata": {
        "id": "bYeU7uCxqyyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code block to override the prompt set in the beginning\n",
        "\n",
        "prompt = 'My new prompt: '"
      ],
      "metadata": {
        "id": "gy4eu0nCrLnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 1000 characters with the model\n",
        "\n",
        "states = None\n",
        "next_char = tf.constant([prompt])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = single_step.generate_one_step(next_char, states = states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "id": "wlhbS7zwq1l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save/load the model"
      ],
      "metadata": {
        "id": "DXecAtcGsJkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load pre-trained Shakespeare model"
      ],
      "metadata": {
        "id": "J3lX1te8xUjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://github.com/shangmingwu/rnn-text-generator/blob/main/data/shakespeare_model_20epochs.zip?raw=true'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('./shakespeare_model.zip', 'wb').write(r.content)\n",
        "with zipfile.ZipFile(\"./shakespeare_model.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"./\")"
      ],
      "metadata": {
        "id": "AGKe_Q56xXPh"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and run the model\n",
        "\n",
        "single_step_loaded = tf.saved_model.load('./single_step')\n",
        "\n",
        "states = None\n",
        "next_char = tf.constant([prompt])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = single_step_loaded.generate_one_step(next_char, states = states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "id": "wGNinFUsxus6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save/load own model"
      ],
      "metadata": {
        "id": "gwjg7r-XxRPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to the current directory, and compress it for download\n",
        "\n",
        "tf.saved_model.save(single_step, './single_step')\n",
        "shutil.make_archive(\"./single_step_saved\", 'zip', \"./single_step\")"
      ],
      "metadata": {
        "id": "yDztGy49sLoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and run the model\n",
        "\n",
        "single_step_loaded = tf.saved_model.load('./single_step')\n",
        "\n",
        "states = None\n",
        "next_char = tf.constant([prompt])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = single_step_loaded.generate_one_step(next_char, states = states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "id": "kac6QlEgsR5z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
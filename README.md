# Generating text with recurrent neural networks

In this repository you'll find a notebook and some data about my explorations with regards to generating text with RNNs.

## Example: Fakespeare

Shakespeare has a relatively large body of writing available, all of it in the public domain: thus, this makes his work an attractive source of data to train an RNN. Here is an example with 20 epochs of training:

```
ROMEO: God sound Lord Marshal.

DUKE OF AUMERLE:
My husband-hardy hearing then Aufidius takes her breast,
Hich the daunt close thy joints forth the hornsome together
To open the fairies: and these's mercy do I;
Stand up, set down, and Tunis, God forbid!
Worst thou, unlawful business in our country,
See the punish here and the best royalty
We have look'd for any language. I did buy,
deficerance we arroub gen to command's; but to
come aqua then my power to grows would bear my country's kind!
Such a pupset and to do me what thou art a
parent. And you in vail to you,
Is it both thee: when we patch this crown to-night
And soon my soul fled by one lighten'd backs,
Send for your beauty-pirce with rue lies hard,
When I am king for restories.
Look you think, that he shall swear, anon,
Cuth gone, this time came from her death: therefore, great Rosaline.

Roman:
An; if a worder from the Duke of Buckingham?

LADY GREY:
'Twixt yourself and your behavior and oder,
Those thinks the leasiun thus uppedienced.
```

Sentences often aren't grammatical and made-up words pop up frequently, but the RNN has learned the script structure of Shakespeare's writing well.

Please see the notebook for sources of data and inspiration, as well as to play with the models for yourself.
